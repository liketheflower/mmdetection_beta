roi_width:-0.713099roi_height-0.0280533
roi_width:0.370985roi_height0.15609
roi_width:-0.464705roi_height-0.18679
roi_width:-0.00769737roi_height-0.16324
roi_width:0.0934538roi_height0.228696
roi_width:0.305462roi_height0.182129
roi_width:-0.100561roi_height-0.173153
roi_width:0.1594roi_height0.52306
roi_width:0.804773roi_height0.0262337
roi_width:0.283562roi_height0.169981
roi_width:-0.230768roi_height-0.0586133
roi_width:-0.104125roi_height-0.121621
roi_width:0.482806roi_height0.150273
roi_width:-0.657883roi_height-0.243602
roi_width:-0.56811roi_height-0.517265
roi_width:-0.489033roi_height-0.654424
roi_width:0.124514roi_height0.234373
roi_width:-0.0565143roi_height-0.280825
roi_width:-0.634849roi_height-0.10182
roi_width:-0.71372roi_height-0.77502
roi_width:0.599777roi_height0.466928
roi_width:-0.0753124roi_height-0.321332
roi_width:0.208868roi_height0.315057
roi_width:-0.343726roi_height-0.530246
roi_width:-0.290816roi_height-0.0260128
roi_width:-0.107109roi_height-0.101872
roi_width:-0.621713roi_height-0.316632
roi_width:-0.103141roi_height-0.544232
roi_width:0.335432roi_height0.107834
roi_width:0.0655712roi_height0.109988
roi_width:0.253163roi_height0.0390278
roi_width:-0.141229roi_height-0.320323
roi_width:0.653973roi_height0.748867
roi_width:-0.256382roi_height-0.363209
roi_width:-0.441215roi_height-0.110934
roi_width:-0.423996roi_height-0.515603
roi_width:-0.780913roi_height-0.0614669
roi_width:0.530256roi_height0.363811
roi_width:-0.0716428roi_height-0.133816
roi_width:0.10573roi_height0.363415
roi_width:-0.265093roi_height-0.299714
roi_width:0.393216roi_height0.0446458
roi_width:-0.00600982roi_height-0.0697283
roi_width:-0.370242roi_height-0.0100433
roi_width:-0.477873roi_height-0.472122
roi_width:-0.327935roi_height-0.359865
roi_width:0.263755roi_height0.206766
roi_width:0.0659674roi_height0.57645
roi_width:0.0708496roi_height0.374584
roi_width:-0.303302roi_height-0.0776506
roi_width:-0.383073roi_height-0.677939
roi_width:0.316322roi_height0.812707
roi_width:0.32557roi_height0.120916
roi_width:-0.189347roi_height-0.422437
roi_width:-0.246475roi_height-0.527564
roi_width:-0.0223967roi_height-0.209625
roi_width:0.341668roi_height0.504787
roi_width:0.186384roi_height0.16825
roi_width:0.124149roi_height0.339291
roi_width:-0.0317717roi_height-0.51192
roi_width:-0.504999roi_height-0.535319
roi_width:-0.096058roi_height-0.360083
roi_width:0.339221roi_height0.557601
roi_width:0.206055roi_height0.776034
roi_width:0.534972roi_height0.403077
roi_width:-0.113858roi_height-0.106782
roi_width:0.381186roi_height0.56515
roi_width:0.628525roi_height0.157146
roi_width:-0.108373roi_height-0.230632
roi_width:-0.181865roi_height-0.211492
roi_width:0.246675roi_height0.0832049
roi_width:0.395199roi_height0.323543
roi_width:0.211048roi_height0.0561413
roi_width:0.205916roi_height0.154776
roi_width:0.426372roi_height0.51312
roi_width:-0.72504roi_height-0.372381
roi_width:-0.410032roi_height-0.163659
roi_width:-0.446906roi_height-0.345298
roi_width:0.347692roi_height0.0641146
roi_width:-0.0589176roi_height-0.0595915
roi_width:0.349911roi_height0.542816
roi_width:-0.0421474roi_height-0.362774
roi_width:-0.141569roi_height-0.652395
roi_width:-0.0411991roi_height-0.283172
roi_width:0.328452roi_height0.0250531
roi_width:-0.423896roi_height-0.258584
roi_width:0.377142roi_height0.356562
roi_width:0.205991roi_height0.0274217
roi_width:0.0978094roi_height0.385559
roi_width:-0.698206roi_height-0.0260692
roi_width:-0.543365roi_height-0.383303
roi_width:-0.431878roi_height-0.0652283
roi_width:-0.394751roi_height-0.105
roi_width:-0.487717roi_height-0.489853
roi_width:0.450049roi_height0.848594
roi_width:0.057888roi_height0.146345
roi_width:0.297064roi_height0.243299
roi_width:0.504248roi_height0.108821
roi_width:0.319284roi_height0.130226
roi_width:-0.0296019roi_height-0.0108346
roi_width:0.099055roi_height0.874378
roi_width:0.349252roi_height0.430238
roi_width:0.569328roi_height0.0486103
roi_width:0.193556roi_height0.146954
roi_width:-0.169225roi_height-0.500136
roi_width:-0.362175roi_height-0.0689517
roi_width:0.25251roi_height0.30126
roi_width:-0.498605roi_height-0.0685858
roi_width:-0.141629roi_height-0.556392
roi_width:0.100576roi_height0.317176
roi_width:0.473399roi_height0.514539
roi_width:0.358644roi_height0.216724
roi_width:-0.787081roi_height-0.352016
roi_width:-0.10466roi_height-0.0127208
roi_width:0.789779roi_height0.0412209
roi_width:-0.420327roi_height-0.548402
roi_width:0.197607roi_height0.25112
roi_width:-0.0299557roi_height-0.646506
roi_width:-0.301999roi_height-0.0962504
roi_width:-0.257156roi_height-0.306851
roi_width:-0.225246roi_height-0.133033
roi_width:-0.243233roi_height-0.231026
roi_width:0.147021roi_height0.172624
roi_width:0.0610138roi_height0.932378
roi_width:0.101788roi_height0.0634183
roi_width:0.119751roi_height0.103785
roi_width:-0.33864roi_height-0.651599
roi_width:0.0245711roi_height0.00315845
roi_width:0.809928roi_height0.142244
roi_width:0.487967roi_height0.253501
roi_width:1.31951roi_height0.619558
roi_width:-0.532983roi_height-0.0428469
roi_width:-0.274759roi_height-0.0440016
roi_width:0.0471527roi_height0.248941
roi_width:-0.128294roi_height-0.305905
roi_width:0.240257roi_height0.458003
roi_width:-0.100819roi_height-0.720285
roi_width:-0.0365476roi_height-0.0524082
roi_width:-0.336726roi_height-0.0854189
roi_width:0.124993roi_height0.0602503
roi_width:-0.577594roi_height-0.222351
roi_width:0.723937roi_height0.159735
roi_width:0.107051roi_height0.394688
roi_width:-0.387375roi_height-0.0965497
roi_width:0.250356roi_height0.161133
roi_width:-0.147879roi_height-0.425432
roi_width:-0.0521154roi_height-0.145022
roi_width:0.55194roi_height0.551066
roi_width:0.284653roi_height0.105755
roi_width:0.0692776roi_height0.578285
roi_width:-0.500385roi_height-0.466032
roi_width:0.0785868roi_height0.0554356
roi_width:0.114462roi_height0.263373
roi_width:0.127951roi_height0.224343
roi_width:-0.302066roi_height-0.121844
roi_width:-0.0253231roi_height-0.191322
roi_width:-0.344725roi_height-0.884707
roi_width:-0.529033roi_height-0.0870355
roi_width:0.223614roi_height0.389985
roi_width:-0.528478roi_height-0.641285
roi_width:-0.219696roi_height-0.46112
roi_width:-0.509699roi_height-0.0316467
roi_width:0.383711roi_height0.130135
roi_width:-0.63546roi_height-0.176949
roi_width:0.298675roi_height0.0500947
roi_width:0.0450732roi_height0.0443798
roi_width:0.0590339roi_height0.0220181
roi_width:0.128832roi_height0.140003
roi_width:-0.222887roi_height-0.0483429
roi_width:0.392684roi_height0.402339
roi_width:-0.383769roi_height-0.0326416
roi_width:-0.114239roi_height-0.506168
roi_width:-0.47408roi_height-0.276712
roi_width:0.550345roi_height0.283259
roi_width:-0.865159roi_height-0.0967208
roi_width:-0.00788173roi_height-0.140702
roi_width:-0.0918284roi_height-0.471339
roi_width:0.0572984roi_height0.131733
roi_width:0.519915roi_height0.733894
roi_width:-0.670676roi_height-0.423621
roi_width:0.00268382roi_height0.17654
roi_width:-0.958139roi_height-0.299647
roi_width:-0.198558roi_height-0.0366051
roi_width:-0.26106roi_height-0.0482144
roi_width:-0.192429roi_height-0.0484519
roi_width:-0.244899roi_height-0.0617114
roi_width:0.498819roi_height0.43036
roi_width:-0.305624roi_height-0.140496
roi_width:-0.409553roi_height-0.0988619
roi_width:0.0617067roi_height0.0578219
roi_width:-0.103971roi_height-0.215485
roi_width:0.157747roi_height0.200322
roi_width:0.0764024roi_height0.430205
roi_width:0.44807roi_height0.460685
roi_width:-0.241434roi_height-0.162897
roi_width:-0.340565roi_height-0.0486877
roi_width:-0.14855roi_height-0.109262
roi_width:0.419793roi_height0.559667
roi_width:0.057071roi_height0.243087
roi_width:0.0872684roi_height0.393487
roi_width:0.00646532roi_height0.115472
roi_width:0.0454557roi_height0.0151608
roi_width:-0.318584roi_height-0.116187
roi_width:-0.305657roi_height-0.0135723
roi_width:0.0683398roi_height0.873911
roi_width:-0.201288roi_height-0.209687
roi_width:0.0977805roi_height0.0089469
roi_width:0.295451roi_height0.174778
roi_width:-0.114828roi_height-0.147513
roi_width:-0.173645roi_height-0.00257069
roi_width:0.111641roi_height0.220091
roi_width:0.252429roi_height0.647489
roi_width:-0.394135roi_height-0.526211
roi_width:0.399702roi_height0.0985276
roi_width:0.154841roi_height0.00313807
roi_width:-0.0629504roi_height-0.506188
roi_width:0.369042roi_height0.673166
roi_width:0.155995roi_height0.0075559
roi_width:-0.409308roi_height-0.0128167
roi_width:0.360009roi_height0.457715
roi_width:0.0205829roi_height0.399457
roi_width:0.191283roi_height0.732724
roi_width:-0.3814roi_height-0.0106498
roi_width:-0.697939roi_height-0.0986266
roi_width:-0.432705roi_height-0.430041
roi_width:0.0805871roi_height0.307219
roi_width:0.273273roi_height0.396211
roi_width:-0.0440846roi_height-0.100579
roi_width:0.21618roi_height0.0557936
roi_width:0.23518roi_height0.301752
roi_width:0.128479roi_height0.118426
roi_width:0.277348roi_height0.17924
roi_width:0.0407834roi_height0.0761927
roi_width:0.686258roi_height0.124051
roi_width:0.347965roi_height0.433999
roi_width:-0.44671roi_height-0.135109
roi_width:-0.0178024roi_height-0.161141
roi_width:-0.246098roi_height-0.209952
roi_width:-0.15319roi_height-0.726719
roi_width:0.174021roi_height0.0553705
roi_width:-0.368335roi_height-0.388245
roi_width:0.252318roi_height0.16055
roi_width:0.0923094roi_height0.0149058
roi_width:-0.220401roi_height-0.526274
roi_width:0.633925roi_height0.0257231
roi_width:-0.0053944roi_height-0.476493
roi_width:0.192992roi_height0.223831
roi_width:-0.21374roi_height-0.0956402
roi_width:0.421322roi_height0.0645495
roi_width:0.250779roi_height0.325462
roi_width:-0.174163roi_height-0.183162
roi_width:-0.692072roi_height-0.024457
roi_width:0.285705roi_height0.144657
roi_width:0.672831roi_height0.657213
roi_width:0.245016roi_height0.493228
roi_width:-0.167669roi_height-0.329446
roi_width:-0.332303roi_height-0.463653
roi_width:0.227285roi_height0.220007
roi_width:-0.117908roi_height-0.304691
roi_width:-0.0256022roi_height-0.220037
roi_width:-0.00841552roi_height-0.420449
roi_width:-0.0200424roi_height-0.26789
roi_width:0.0631899roi_height0.0789424
roi_width:-0.260798roi_height-0.18129
roi_width:0.322392roi_height0.296228
roi_width:0.16962roi_height0.447445
roi_width:-0.58272roi_height-0.138685
roi_width:0.377573roi_height0.359702
roi_width:0.136413roi_height0.268101
roi_width:0.19229roi_height0.230298
roi_width:0.595798roi_height0.144886
roi_width:0.24989roi_height0.620204
roi_width:-1.16631roi_height-0.0753812
roi_width:0.229516roi_height0.510457
roi_width:0.196289roi_height0.188819
roi_width:-0.598196roi_height-0.111201
roi_width:-0.0220755roi_height-0.0276213
roi_width:-0.0240837roi_height-0.128874
roi_width:-0.541555roi_height-0.0250106
roi_width:-0.0956533roi_height-0.0639448
roi_width:-0.317731roi_height-0.272289
roi_width:-0.804101roi_height-0.451205
roi_width:-0.351979roi_height-0.174722
roi_width:0.195292roi_height0.0725431
roi_width:-0.284654roi_height-0.393023
roi_width:0.50575roi_height0.318094
roi_width:-0.166956roi_height-0.118417
roi_width:0.0215912roi_height0.183007
roi_width:-0.54846roi_height-0.605179
roi_width:-0.389629roi_height-0.300237
roi_width:-0.246045roi_height-0.287807
roi_width:-0.178042roi_height-0.387182
roi_width:0.444731roi_height0.000114441
roi_width:0.42255roi_height0.212356
roi_width:-0.169842roi_height-0.674054
roi_width:-0.659298roi_height-0.256156
roi_width:0.355941roi_height0.251169
roi_width:0.35572roi_height0.547887
roi_width:-0.00631851roi_height-0.0702532
roi_width:-0.255281roi_height-0.735282
roi_width:0.073688roi_height0.474934
roi_width:0.398051roi_height0.19536
roi_width:-0.0788465roi_height-0.401013
roi_width:-0.175261roi_height-0.094703
roi_width:-0.0597407roi_height-0.228341
roi_width:-0.0200735roi_height-0.165084
roi_width:0.648654roi_height0.50619
roi_width:-0.209933roi_height-0.141203
roi_width:0.378284roi_height0.119871
roi_width:-0.0163493roi_height-0.408597
roi_width:-0.0695988roi_height-0.00823367
roi_width:-0.276441roi_height-0.127543
roi_width:-0.231253roi_height-0.390037
roi_width:0.255843roi_height0.127478
roi_width:-0.653499roi_height-0.0824966
roi_width:0.225478roi_height0.138738
roi_width:-0.0760048roi_height-0.243077
roi_width:-0.642075roi_height-0.146561
roi_width:0.0264452roi_height0.186109
roi_width:-0.28813roi_height-0.179124
roi_width:-0.121822roi_height-0.931071
roi_width:-0.54176roi_height-0.262485
roi_width:-0.198007roi_height-0.0217339
roi_width:-0.265876roi_height-0.261868
roi_width:-0.219365roi_height-0.630426
roi_width:0.127686roi_height0.456357
roi_width:-0.0651815roi_height-0.273943
roi_width:-0.00165015roi_height-0.390027
roi_width:0.441299roi_height0.233417
roi_width:0.00240341roi_height0.29372
roi_width:-0.425804roi_height-0.136479
roi_width:0.184964roi_height0.240661
roi_width:-0.128034roi_height-0.201495
roi_width:0.0929524roi_height0.149402
roi_width:0.564597roi_height0.535937
roi_width:0.190904roi_height0.385727
roi_width:0.134401roi_height0.0474813
roi_width:-0.476454roi_height-0.667346
roi_width:0.306796roi_height0.268923
roi_width:0.214531roi_height0.278389
roi_width:-0.0193024roi_height-0.677077
roi_width:0.151331roi_height0.129101
roi_width:0.453249roi_height0.247258
roi_width:-0.0179093roi_height-0.103965
roi_width:0.274279roi_height0.181773
roi_width:0.0743545roi_height0.0693201
roi_width:-0.523735roi_height-0.101169
roi_width:0.581568roi_height0.20445
roi_width:-0.195664roi_height-0.0802751
roi_width:0.343765roi_height0.285436
roi_width:0.895798roi_height0.184251
roi_width:0.504219roi_height0.117754
roi_width:-0.30555roi_height-0.231714
roi_width:0.0610241roi_height0.650428
roi_width:0.284382roi_height0.426011
roi_width:0.26298roi_height0.464647
roi_width:0.292567roi_height0.422257
roi_width:0.159386roi_height0.569382
roi_width:-0.189866roi_height-0.0496351
roi_width:0.0487801roi_height0.172859
roi_width:0.32927roi_height0.393914
roi_width:0.574381roi_height0.349742
roi_width:-0.490217roi_height-0.0568839
roi_width:0.968326roi_height0.485819
roi_width:-0.0858809roi_height-0.0999075
roi_width:-0.452979roi_height-0.162933
roi_width:-0.0887817roi_height-0.258838
roi_width:-0.116379roi_height-0.148002
roi_width:-0.0732821roi_height-0.490505
roi_width:-0.840289roi_height-0.524557
roi_width:-0.167474roi_height-0.48929
roi_width:-0.0159524roi_height-0.254848
roi_width:-0.366452roi_height-0.408565
roi_width:-0.0745674roi_height-0.064887
roi_width:-0.112774roi_height-0.0150205
roi_width:0.558093roi_height0.193289
roi_width:-0.0743501roi_height-0.237509
roi_width:-0.298498roi_height-0.22154
roi_width:-0.188283roi_height-0.506089
roi_width:-0.143393roi_height-0.432873
roi_width:0.399107roi_height0.431448
roi_width:0.122845roi_height0.28112
roi_width:0.0385116roi_height0.123284
roi_width:0.0231963roi_height0.507962
roi_width:-0.297307roi_height-0.134223
roi_width:-0.017251roi_height-0.650531
roi_width:0.0923361roi_height0.0452428
roi_width:0.263641roi_height0.213977
roi_width:0.276752roi_height0.109551
roi_width:-0.0819133roi_height-0.0819926
roi_width:-0.177043roi_height-0.618419
roi_width:-0.526057roi_height-0.142747
roi_width:-0.163506roi_height-0.0180387
roi_width:-0.0310007roi_height-0.267553
roi_width:0.280876roi_height0.982176
roi_width:-0.748315roi_height-0.0940157
roi_width:0.327799roi_height0.271168
roi_width:0.185553roi_height0.0391699
roi_width:0.14294roi_height0.268158
roi_width:0.371512roi_height0.0628453
roi_width:-0.144557roi_height-0.643422
roi_width:0.130374roi_height0.0775482
roi_width:-0.212571roi_height-0.261375
roi_width:0.292033roi_height0.315115
roi_width:-0.0797884roi_height-0.00274909
roi_width:0.0547189roi_height0.544463
roi_width:-0.273052roi_height-0.0828118
roi_width:-0.302867roi_height-0.0873682
roi_width:-0.444764roi_height-0.482464
roi_width:0.197406roi_height0.451769
roi_width:-0.0584494roi_height-0.0362364
roi_width:-0.571302roi_height-1.1051
roi_width:0.102146roi_height0.0750698
roi_width:0.667875roi_height0.0171642
roi_width:-0.0463608roi_height-0.352351
roi_width:-0.300515roi_height-0.562695
roi_width:0.180219roi_height0.620337
roi_width:-0.206597roi_height-0.34727
roi_width:0.306085roi_height0.0868917
roi_width:0.348593roi_height0.0954713
roi_width:0.299096roi_height0.226773
roi_width:0.0969978roi_height0.368652
roi_width:-0.189358roi_height-0.550758
roi_width:0.109945roi_height0.223733
roi_width:0.780784roi_height0.206079
roi_width:-0.346563roi_height-0.0777245
roi_width:-0.451259roi_height-0.198343
roi_width:-0.141235roi_height-0.533001
roi_width:-0.757525roi_height-0.335584
roi_width:0.145715roi_height0.0179178
roi_width:-0.331114roi_height-0.274833
roi_width:-0.192759roi_height-0.0557276
roi_width:0.0880749roi_height0.120379
roi_width:0.34061roi_height0.391607
roi_width:-0.347606roi_height-0.371691
roi_width:0.1629roi_height0.30551
roi_width:-0.0594808roi_height-0.154677
roi_width:0.219451roi_height0.0411364
roi_width:-0.0983464roi_height-0.560532
roi_width:-0.598952roi_height-0.13739
roi_width:0.0265611roi_height0.227857
roi_width:-0.0617283roi_height-0.170139
roi_width:-0.469083roi_height-0.0509607
roi_width:-0.0783819roi_height-0.0991622
roi_width:0.506498roi_height0.104665
roi_width:0.367219roi_height0.198544
roi_width:-0.0672461roi_height-0.464401
roi_width:-0.151393roi_height-0.000800729
roi_width:0.554588roi_height0.341949
roi_width:-0.337927roi_height-0.394208
roi_width:0.513299roi_height0.473499
roi_width:0.21531roi_height0.268122
roi_width:0.314156roi_height0.106128
roi_width:-0.158051roi_height-0.02257
roi_width:-0.059304roi_height-0.145858
roi_width:-0.512594roi_height-0.011479
roi_width:-0.153418roi_height-0.0462833
roi_width:0.355565roi_height0.255037
roi_width:-0.0780208roi_height-0.078391
roi_width:0.0608869roi_height0.188934
roi_width:0.432666roi_height0.179124
roi_width:0.0176827roi_height0.201368
roi_width:-0.281713roi_height-0.200116
roi_width:0.1261roi_height0.0560809
roi_width:-0.0723251roi_height-0.566576
roi_width:-0.246747roi_height-0.0487499
roi_width:-0.21885roi_height-0.393085
roi_width:-0.160793roi_height-0.363229
roi_width:-0.642743roi_height-0.105252
roi_width:0.338069roi_height0.409587
roi_width:-0.165853roi_height-0.358731
roi_width:-0.713099roi_height-0.0280533
roi_width:0.370985roi_height0.15609
roi_width:-0.464705roi_height-0.18679
roi_width:-0.00769737roi_height-0.16324
roi_width:0.0934538roi_height0.228696
roi_width:0.305462roi_height0.182129
roi_width:-0.100561roi_height-0.173153
roi_width:0.1594roi_height0.52306
roi_width:0.804773roi_height0.0262337
roi_width:0.283562roi_height0.169981
roi_width:-0.230768roi_height-0.0586133
roi_width:-0.104125roi_height-0.121621
roi_width:0.482806roi_height0.150273
roi_width:-0.657883roi_height-0.243602
roi_width:-0.56811roi_height-0.517265
roi_width:-0.489033roi_height-0.654424
roi_width:0.124514roi_height0.234373
roi_width:-0.0565143roi_height-0.280825
roi_width:-0.634849roi_height-0.10182
roi_width:-0.71372roi_height-0.77502
roi_width:0.599777roi_height0.466928
roi_width:-0.0753124roi_height-0.321332
roi_width:0.208868roi_height0.315057
roi_width:-0.343726roi_height-0.530246
roi_width:-0.290816roi_height-0.0260128
roi_width:-0.107109roi_height-0.101872
roi_width:-0.621713roi_height-0.316632
roi_width:-0.103141roi_height-0.544232
roi_width:0.335432roi_height0.107834
roi_width:0.0655712roi_height0.109988
roi_width:0.253163roi_height0.0390278
roi_width:-0.141229roi_height-0.320323
roi_width:0.653973roi_height0.748867
roi_width:-0.256382roi_height-0.363209
roi_width:-0.441215roi_height-0.110934
roi_width:-0.423996roi_height-0.515603
roi_width:-0.780913roi_height-0.0614669
roi_width:0.530256roi_height0.363811
roi_width:-0.0716428roi_height-0.133816
roi_width:0.10573roi_height0.363415
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: module AdaptivePadding is treated as a zero-op.
Warning: module LayerNorm is treated as a zero-op.
Warning: module PatchEmbed is treated as a zero-op.
Warning: module Dropout is treated as a zero-op.
Warning: module Softmax is treated as a zero-op.
Warning: module WindowMSA is treated as a zero-op.
Warning: module DropPath is treated as a zero-op.
Warning: module ShiftWindowMSA is treated as a zero-op.
Warning: module Linear is treated as a zero-op.
Warning: module Sequential is treated as a zero-op.
Warning: module FFN is treated as a zero-op.
Warning: module SwinBlock is treated as a zero-op.
Warning: module ModuleList is treated as a zero-op.
Warning: module Unfold is treated as a zero-op.
Warning: module PatchMerging is treated as a zero-op.
Warning: module SwinBlockSequence is treated as a zero-op.
Warning: module SwinTransformer is treated as a zero-op.
Warning: module ConvModule is treated as a zero-op.
Warning: module FPN is treated as a zero-op.
Warning: module CrossEntropyLoss is treated as a zero-op.
Warning: module L1Loss is treated as a zero-op.
Warning: module RPNHead is treated as a zero-op.
Warning: module RoIAlign is treated as a zero-op.
Warning: module SingleRoIExtractor is treated as a zero-op.
Warning: module Shared2FCBBoxHead is treated as a zero-op.
Warning: module ConvTranspose2d is treated as a zero-op.
Warning: module Conv2d is treated as a zero-op.
Warning: module FCNMaskHead is treated as a zero-op.
Warning: module StandardRoIHead is treated as a zero-op.
Warning: module MaskRCNN is treated as a zero-op.
MaskRCNN(
  37.28 M, 53.947% Params, 209.87 GMac, 100.000% MACs, 
  (backbone): SwinTransformer(
    17.29 M, 25.022% Params, 58.46 GMac, 27.856% MACs, 
    (patch_embed): PatchEmbed(
      4.7 k, 0.007% Params, 263.42 MMac, 0.126% MACs, 
      (adap_padding): AdaptivePadding(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
      (projection): Conv2d(4.7 k, 0.007% Params, 263.42 MMac, 0.126% MACs, 3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (96,), eps=1e-05, elementwise_affine=True)
    )
    (drop_after_pos): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
    (stages): ModuleList(
      17.29 M, 25.015% Params, 58.2 GMac, 27.731% MACs, 
      (0): SwinBlockSequence(
        148.22 k, 0.214% Params, 5.31 GMac, 2.530% MACs, 
        (blocks): ModuleList(
          74.5 k, 0.108% Params, 4.28 GMac, 2.038% MACs, 
          (0): SwinBlock(
            37.25 k, 0.054% Params, 2.14 GMac, 1.019% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              37.25 k, 0.054% Params, 2.1 GMac, 0.998% MACs, 
              (w_msa): WindowMSA(
                37.25 k, 0.054% Params, 2.1 GMac, 0.998% MACs, 
                (qkv): Linear(27.94 k, 0.040% Params, 1.57 GMac, 0.749% MACs, in_features=96, out_features=288, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(9.31 k, 0.013% Params, 523.84 MMac, 0.250% MACs, in_features=96, out_features=96, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 43.01 MMac, 0.020% MACs, 
              (activate): GELU(0, 0.000% Params, 21.5 MMac, 0.010% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 21.5 MMac, 0.010% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 21.5 MMac, 0.010% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=96, out_features=384, bias=True)
                  (1): GELU(0, 0.000% Params, 21.5 MMac, 0.010% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=96, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (1): SwinBlock(
            37.25 k, 0.054% Params, 2.14 GMac, 1.019% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              37.25 k, 0.054% Params, 2.1 GMac, 0.998% MACs, 
              (w_msa): WindowMSA(
                37.25 k, 0.054% Params, 2.1 GMac, 0.998% MACs, 
                (qkv): Linear(27.94 k, 0.040% Params, 1.57 GMac, 0.749% MACs, in_features=96, out_features=288, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(9.31 k, 0.013% Params, 523.84 MMac, 0.250% MACs, in_features=96, out_features=96, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 43.01 MMac, 0.020% MACs, 
              (activate): GELU(0, 0.000% Params, 21.5 MMac, 0.010% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 21.5 MMac, 0.010% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 21.5 MMac, 0.010% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=96, out_features=384, bias=True)
                  (1): GELU(0, 0.000% Params, 21.5 MMac, 0.010% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=96, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
        )
        (downsample): PatchMerging(
          73.73 k, 0.107% Params, 1.03 GMac, 0.492% MACs, 
          (adap_padding): AdaptivePadding(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          (sampler): Unfold(0, 0.000% Params, 0.0 Mac, 0.000% MACs, kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
          (norm): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(73.73 k, 0.107% Params, 1.03 GMac, 0.492% MACs, in_features=384, out_features=192, bias=False)
        )
      )
      (1): SwinBlockSequence(
        591.36 k, 0.856% Params, 5.41 GMac, 2.578% MACs, 
        (blocks): ModuleList(
          296.45 k, 0.429% Params, 4.38 GMac, 2.086% MACs, 
          (0): SwinBlock(
            148.22 k, 0.214% Params, 2.19 GMac, 1.043% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (192,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              148.22 k, 0.214% Params, 2.17 GMac, 1.033% MACs, 
              (w_msa): WindowMSA(
                148.22 k, 0.214% Params, 2.17 GMac, 1.033% MACs, 
                (qkv): Linear(111.17 k, 0.161% Params, 1.63 GMac, 0.775% MACs, in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(37.06 k, 0.054% Params, 541.9 MMac, 0.258% MACs, in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (192,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 21.5 MMac, 0.010% MACs, 
              (activate): GELU(0, 0.000% Params, 10.75 MMac, 0.005% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=192, out_features=768, bias=True)
                  (1): GELU(0, 0.000% Params, 10.75 MMac, 0.005% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=768, out_features=192, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (1): SwinBlock(
            148.22 k, 0.214% Params, 2.19 GMac, 1.043% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (192,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              148.22 k, 0.214% Params, 2.17 GMac, 1.033% MACs, 
              (w_msa): WindowMSA(
                148.22 k, 0.214% Params, 2.17 GMac, 1.033% MACs, 
                (qkv): Linear(111.17 k, 0.161% Params, 1.63 GMac, 0.775% MACs, in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(37.06 k, 0.054% Params, 541.9 MMac, 0.258% MACs, in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (192,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 21.5 MMac, 0.010% MACs, 
              (activate): GELU(0, 0.000% Params, 10.75 MMac, 0.005% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=192, out_features=768, bias=True)
                  (1): GELU(0, 0.000% Params, 10.75 MMac, 0.005% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=768, out_features=192, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
        )
        (downsample): PatchMerging(
          294.91 k, 0.427% Params, 1.03 GMac, 0.492% MACs, 
          (adap_padding): AdaptivePadding(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          (sampler): Unfold(0, 0.000% Params, 0.0 Mac, 0.000% MACs, kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
          (norm): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (768,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(294.91 k, 0.427% Params, 1.03 GMac, 0.492% MACs, in_features=768, out_features=384, bias=False)
        )
      )
      (2): SwinBlockSequence(
        11.82 M, 17.109% Params, 42.84 GMac, 20.414% MACs, 
        (blocks): ModuleList(
          10.64 M, 15.402% Params, 41.81 GMac, 19.923% MACs, 
          (0): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (1): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (2): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (3): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (4): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (5): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (6): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (7): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (8): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (9): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (10): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (11): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (12): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (13): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (14): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (15): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (16): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (17): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
        )
        (downsample): PatchMerging(
          1.18 M, 1.707% Params, 1.03 GMac, 0.492% MACs, 
          (adap_padding): AdaptivePadding(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          (sampler): Unfold(0, 0.000% Params, 0.0 Mac, 0.000% MACs, kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
          (norm): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (1536,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(1.18 M, 1.707% Params, 1.03 GMac, 0.492% MACs, in_features=1536, out_features=768, bias=False)
        )
      )
      (3): SwinBlockSequence(
        4.72 M, 6.836% Params, 4.63 GMac, 2.209% MACs, 
        (blocks): ModuleList(
          4.72 M, 6.836% Params, 4.63 GMac, 2.209% MACs, 
          (0): SwinBlock(
            2.36 M, 3.418% Params, 2.32 GMac, 1.104% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              2.36 M, 3.418% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                2.36 M, 3.418% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(1.77 M, 2.564% Params, 1.73 GMac, 0.826% MACs, in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(590.59 k, 0.855% Params, 578.03 MMac, 0.275% MACs, in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
              (activate): GELU(0, 0.000% Params, 2.69 MMac, 0.001% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 2.69 MMac, 0.001% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 2.69 MMac, 0.001% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=768, out_features=3072, bias=True)
                  (1): GELU(0, 0.000% Params, 2.69 MMac, 0.001% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=3072, out_features=768, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (1): SwinBlock(
            2.36 M, 3.418% Params, 2.32 GMac, 1.104% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              2.36 M, 3.418% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                2.36 M, 3.418% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(1.77 M, 2.564% Params, 1.73 GMac, 0.826% MACs, in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(590.59 k, 0.855% Params, 578.03 MMac, 0.275% MACs, in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
              (activate): GELU(0, 0.000% Params, 2.69 MMac, 0.001% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 2.69 MMac, 0.001% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 2.69 MMac, 0.001% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=768, out_features=3072, bias=True)
                  (1): GELU(0, 0.000% Params, 2.69 MMac, 0.001% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=3072, out_features=768, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
        )
      )
    )
    (norm0): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (96,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (192,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
    (norm3): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (768,), eps=1e-05, elementwise_affine=True)
  )
  (neck): FPN(
    2.73 M, 3.950% Params, 46.49 GMac, 22.150% MACs, 
    (lateral_convs): ModuleList(
      369.66 k, 0.535% Params, 2.6 GMac, 1.239% MACs, 
      (0): ConvModule(
        24.83 k, 0.036% Params, 1.39 GMac, 0.663% MACs, 
        (conv): Conv2d(24.83 k, 0.036% Params, 1.39 GMac, 0.663% MACs, 96, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        49.41 k, 0.071% Params, 691.71 MMac, 0.330% MACs, 
        (conv): Conv2d(49.41 k, 0.071% Params, 691.71 MMac, 0.330% MACs, 192, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        98.56 k, 0.143% Params, 344.96 MMac, 0.164% MACs, 
        (conv): Conv2d(98.56 k, 0.143% Params, 344.96 MMac, 0.164% MACs, 384, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ConvModule(
        196.86 k, 0.285% Params, 172.26 MMac, 0.082% MACs, 
        (conv): Conv2d(196.86 k, 0.285% Params, 172.26 MMac, 0.082% MACs, 768, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      2.36 M, 3.415% Params, 43.89 GMac, 20.912% MACs, 
      (0): ConvModule(
        590.08 k, 0.854% Params, 33.04 GMac, 15.745% MACs, 
        (conv): Conv2d(590.08 k, 0.854% Params, 33.04 GMac, 15.745% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        590.08 k, 0.854% Params, 8.26 GMac, 3.936% MACs, 
        (conv): Conv2d(590.08 k, 0.854% Params, 8.26 GMac, 3.936% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        590.08 k, 0.854% Params, 2.07 GMac, 0.984% MACs, 
        (conv): Conv2d(590.08 k, 0.854% Params, 2.07 GMac, 0.984% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        590.08 k, 0.854% Params, 516.32 MMac, 0.246% MACs, 
        (conv): Conv2d(590.08 k, 0.854% Params, 516.32 MMac, 0.246% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (rpn_head): RPNHead(
    593.93 k, 0.859% Params, 44.31 GMac, 21.114% MACs, 
    (loss_cls): CrossEntropyLoss(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
    (loss_bbox): L1Loss(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
    (rpn_conv): Conv2d(590.08 k, 0.854% Params, 44.03 GMac, 20.977% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (rpn_cls): Conv2d(771, 0.001% Params, 57.52 MMac, 0.027% MACs, 256, 3, kernel_size=(1, 1), stride=(1, 1))
    (rpn_reg): Conv2d(3.08 k, 0.004% Params, 230.09 MMac, 0.110% MACs, 256, 12, kernel_size=(1, 1), stride=(1, 1))
  )
  init_cfg={'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}
  (roi_head): StandardRoIHead(
    16.67 M, 24.116% Params, 60.61 GMac, 28.879% MACs, 
    (bbox_roi_extractor): SingleRoIExtractor(
      0, 0.000% Params, 0.0 Mac, 0.000% MACs, 
      (roi_layers): ModuleList(
        0, 0.000% Params, 0.0 Mac, 0.000% MACs, 
        (0): RoIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
        (1): RoIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
        (2): RoIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
        (3): RoIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
      )
    )
    (bbox_head): Shared2FCBBoxHead(
      14.31 M, 20.701% Params, 14.31 GMac, 6.817% MACs, 
      (loss_cls): CrossEntropyLoss(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
      (loss_bbox): L1Loss(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
      (fc_cls): Linear(83.03 k, 0.120% Params, 82.94 MMac, 0.040% MACs, in_features=1024, out_features=81, bias=True)
      (fc_reg): Linear(328.0 k, 0.475% Params, 327.68 MMac, 0.156% MACs, in_features=1024, out_features=320, bias=True)
      (shared_convs): ModuleList(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
      (shared_fcs): ModuleList(
        13.9 M, 20.106% Params, 13.89 GMac, 6.620% MACs, 
        (0): Linear(12.85 M, 18.587% Params, 12.85 GMac, 6.120% MACs, in_features=12544, out_features=1024, bias=True)
        (1): Linear(1.05 M, 1.519% Params, 1.05 GMac, 0.500% MACs, in_features=1024, out_features=1024, bias=True)
      )
      (cls_convs): ModuleList(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
      (cls_fcs): ModuleList(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
      (reg_convs): ModuleList(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
      (reg_fcs): ModuleList(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
      (relu): ReLU(0, 0.000% Params, 2.05 MMac, 0.001% MACs, inplace=True)
    )
    init_cfg=[{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}, {'type': 'Xavier', 'distribution': 'uniform', 'override': [{'name': 'shared_fcs'}, {'name': 'cls_fcs'}, {'name': 'reg_fcs'}]}]
    (mask_roi_extractor): SingleRoIExtractor(
      0, 0.000% Params, 0.0 Mac, 0.000% MACs, 
      (roi_layers): ModuleList(
        0, 0.000% Params, 0.0 Mac, 0.000% MACs, 
        (0): RoIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
        (1): RoIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
        (2): RoIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
        (3): RoIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
      )
    )
    (mask_head): FCNMaskHead(
      2.36 M, 3.415% Params, 46.3 GMac, 22.062% MACs, 
      (loss_mask): CrossEntropyLoss(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
      (convs): ModuleList(
        2.36 M, 3.415% Params, 46.28 GMac, 22.053% MACs, 
        (0): ConvModule(
          590.08 k, 0.854% Params, 11.57 GMac, 5.513% MACs, 
          (conv): Conv2d(590.08 k, 0.854% Params, 11.57 GMac, 5.511% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): ReLU(0, 0.000% Params, 5.02 MMac, 0.002% MACs, inplace=True)
        )
        (1): ConvModule(
          590.08 k, 0.854% Params, 11.57 GMac, 5.513% MACs, 
          (conv): Conv2d(590.08 k, 0.854% Params, 11.57 GMac, 5.511% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): ReLU(0, 0.000% Params, 5.02 MMac, 0.002% MACs, inplace=True)
        )
        (2): ConvModule(
          590.08 k, 0.854% Params, 11.57 GMac, 5.513% MACs, 
          (conv): Conv2d(590.08 k, 0.854% Params, 11.57 GMac, 5.511% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): ReLU(0, 0.000% Params, 5.02 MMac, 0.002% MACs, inplace=True)
        )
        (3): ConvModule(
          590.08 k, 0.854% Params, 11.57 GMac, 5.513% MACs, 
          (conv): Conv2d(590.08 k, 0.854% Params, 11.57 GMac, 5.511% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): ReLU(0, 0.000% Params, 5.02 MMac, 0.002% MACs, inplace=True)
        )
      )
      (upsample): ConvTranspose2d(0, 0.000% Params, 0.0 Mac, 0.000% MACs, 256, 256, kernel_size=(2, 2), stride=(2, 2))
      (conv_logits): Conv2d(0, 0.000% Params, 0.0 Mac, 0.000% MACs, 256, 80, kernel_size=(1, 1), stride=(1, 1))
      (relu): ReLU(0, 0.000% Params, 20.07 MMac, 0.010% MACs, inplace=True)
    )
  )
)roi_width:-0.317488roi_height-0.392802
roi_width:-0.0175961roi_height-0.493412
roi_width:-0.715035roi_height-0.0660552
roi_width:-0.388039roi_height-0.0588973
roi_width:0.399482roi_height0.168232
roi_width:0.222885roi_height0.100168
roi_width:0.156464roi_height0.00690687
roi_width:-0.267699roi_height-0.156143
roi_width:0.147979roi_height0.248031
roi_width:0.233456roi_height0.408586
roi_width:-0.00720209roi_height-0.447496
roi_width:-0.467951roi_height-0.777371
roi_width:-0.311464roi_height-0.0994589
roi_width:0.214951roi_height0.8584
roi_width:0.653153roi_height0.222348
roi_width:-0.0973344roi_height-0.435282
roi_width:-0.388845roi_height-0.0570643
roi_width:0.0940124roi_height0.436449
roi_width:0.524018roi_height0.32139
roi_width:0.25007roi_height0.0314492
roi_width:-0.391419roi_height-0.173375
roi_width:0.197935roi_height0.225283
roi_width:-0.53991roi_height-0.0569916
roi_width:0.434492roi_height0.123476
roi_width:0.0351439roi_height0.44046
roi_width:-0.347214roi_height-0.00239208
roi_width:0.224764roi_height0.732544
roi_width:-0.105223roi_height-0.0838021
roi_width:0.0996937roi_height0.156132
roi_width:-0.498101roi_height-0.11056
roi_width:-0.23077roi_height-0.180307
roi_width:0.117199roi_height0.194437
roi_width:-0.0796048roi_height-0.68464
roi_width:-0.307048roi_height-0.569808
roi_width:-0.114038roi_height-0.381857
roi_width:-0.824798roi_height-0.0431581
roi_width:-0.318624roi_height-0.329799
roi_width:-0.112112roi_height-0.593551
roi_width:-0.792285roi_height-0.378192
roi_width:-0.694669roi_height-0.00613183
roi_width:-0.105673roi_height-1.01601
roi_width:-0.608046roi_height-0.022254
roi_width:0.179664roi_height0.38544
roi_width:-0.250911roi_height-0.547025
roi_width:-0.469374roi_height-0.297748
roi_width:-0.435052roi_height-0.0093165
roi_width:0.00687718roi_height0.316287
roi_width:-0.142045roi_height-0.0828197
roi_width:-0.172616roi_height-0.16279
roi_width:0.0497229roi_height0.277094
roi_width:0.215476roi_height0.279031
roi_width:0.377947roi_height0.791914
roi_width:0.546824roi_height0.232143
roi_width:-0.374527roi_height-0.353756
roi_width:0.552506roi_height0.0941462
roi_width:0.274384roi_height0.304261
roi_width:-0.197802roi_height-0.18131
roi_width:0.360199roi_height0.20318
roi_width:0.166544roi_height0.0839657
roi_width:0.166533roi_height0.356367
roi_width:0.181166roi_height0.253772
roi_width:-0.218857roi_height-0.493597
roi_width:0.565927roi_height0.556377
roi_width:-0.500614roi_height-0.11449
roi_width:0.466152roi_height0.326618
roi_width:0.0152944roi_height0.459212
roi_width:0.790757roi_height0.186106
roi_width:-0.115022roi_height-0.303863
roi_width:-0.213763roi_height-0.277388
roi_width:0.926114roi_height0.0665581
roi_width:-0.0356465roi_height-0.238754
roi_width:0.123253roi_height0.183569
roi_width:0.089093roi_height0.297144
roi_width:0.079341roi_height0.248926
roi_width:0.0250933roi_height0.211215
roi_width:-0.0758975roi_height-0.336082
roi_width:-0.762458roi_height-0.495967
roi_width:-0.511831roi_height-0.0180911
roi_width:0.688768roi_height0.303052
roi_width:-0.351603roi_height-0.217965
roi_width:0.554747roi_height0.146085
roi_width:-0.233788roi_height-0.163407
roi_width:0.535627roi_height0.247452
roi_width:-0.220069roi_height-0.0370997
roi_width:0.38758roi_height0.412923
roi_width:0.477535roi_height0.369454
roi_width:-0.229287roi_height-0.392862
roi_width:0.134637roi_height0.195952
roi_width:0.23161roi_height0.0243076
roi_width:0.566277roi_height0.3384
roi_width:0.514288roi_height0.079887
roi_width:-0.105643roi_height-0.100737
roi_width:-0.358013roi_height-0.106751
roi_width:0.101138roi_height0.320844
roi_width:0.611731roi_height0.58904
roi_width:-0.243025roi_height-0.187548
roi_width:-0.234575roi_height-0.215601
roi_width:-0.100288roi_height-0.0950071
roi_width:0.449569roi_height0.344728
roi_width:0.0440961roi_height0.499389
roi_width:0.378619roi_height0.175297
roi_width:-0.0887747roi_height-0.236403
roi_width:0.291392roi_height0.182906
roi_width:0.366745roi_height0.0996829
roi_width:0.267929roi_height0.21666
roi_width:0.191355roi_height0.153211
roi_width:-0.0418362roi_height-0.00492823
roi_width:0.457036roi_height0.180803
roi_width:-0.693839roi_height-0.0855721
roi_width:-0.752528roi_height-0.481081
roi_width:0.277129roi_height0.224729
roi_width:-0.235541roi_height-0.0843867
roi_width:-0.472191roi_height-0.223271
roi_width:-0.360243roi_height-0.397356
roi_width:-0.0791583roi_height-0.0114374
roi_width:-0.45985roi_height-0.456083
roi_width:0.122495roi_height0.639678
roi_width:-0.173616roi_height-0.323288
roi_width:0.298685roi_height0.154637
roi_width:0.200659roi_height0.00771272
roi_width:-0.0607799roi_height-0.282193
roi_width:-0.0825983roi_height-0.142628
roi_width:-0.170669roi_height-0.245663
roi_width:0.373755roi_height0.352045
roi_width:-0.176402roi_height-0.399468
roi_width:-0.562082roi_height-0.0775577
roi_width:-0.39381roi_height-0.232696
roi_width:-0.45566roi_height-0.301287
roi_width:-0.198562roi_height-0.22243
roi_width:0.110193roi_height0.00571531
roi_width:0.81788roi_height0.226057
roi_width:0.401746roi_height0.103198
roi_width:0.260838roi_height0.0968843
roi_width:-0.642876roi_height-0.317733
roi_width:-0.451456roi_height-0.466128
roi_width:-0.119506roi_height-0.637234
roi_width:-0.023604roi_height-0.365434
roi_width:-0.373257roi_height-0.459487
roi_width:0.165828roi_height0.0359207
roi_width:0.426148roi_height0.121902
roi_width:-0.0549406roi_height-0.385312
roi_width:0.0296236roi_height0.277984
roi_width:-0.0315243roi_height-0.254432
roi_width:-0.0234725roi_height-0.230592
roi_width:0.0659416roi_height0.605518
roi_width:0.0836906roi_height0.492456
roi_width:-0.164051roi_height-0.552417
roi_width:-0.246104roi_height-0.697318
roi_width:-0.535097roi_height-0.3807
roi_width:-0.108644roi_height-0.0359179
roi_width:0.161895roi_height0.496382
roi_width:0.18859roi_height0.363653
roi_width:0.321238roi_height0.139623
roi_width:0.176288roi_height0.201986
roi_width:0.0319264roi_height0.080069
roi_width:0.0916691roi_height0.844555
roi_width:0.562243roi_height0.0561773
roi_width:0.248793roi_height0.252463
roi_width:-0.242329roi_height-0.0337019
roi_width:-0.210127roi_height-0.247026
roi_width:0.422476roi_height0.0742626
roi_width:-0.577781roi_height-0.00474936
roi_width:-0.0289928roi_height-0.102735
roi_width:0.552011roi_height0.505203
roi_width:-0.150516roi_height-0.0184138
roi_width:0.471934roi_height0.319362
roi_width:-0.152762roi_height-0.103016
roi_width:0.152086roi_height0.513057
roi_width:0.563237roi_height0.259367
roi_width:-0.125957roi_height-0.0356723
roi_width:-0.591927roi_height-0.284648
roi_width:-0.571936roi_height-0.221975
roi_width:-0.0712937roi_height-0.368642
roi_width:-0.275862roi_height-0.183213
roi_width:0.122294roi_height0.445895
roi_width:-0.0591389roi_height-0.227986
roi_width:0.140267roi_height0.0391424
roi_width:-0.623317roi_height-0.345458
roi_width:-0.432212roi_height-0.553114
roi_width:-0.166952roi_height-0.172756
roi_width:0.069076roi_height0.249042
roi_width:0.235599roi_height0.202358
roi_width:0.100228roi_height0.081809
roi_width:-0.190029roi_height-0.268624
roi_width:0.754575roi_height0.475655
roi_width:-0.644938roi_height-0.641283
roi_width:-0.80619roi_height-0.946064
roi_width:-0.101328roi_height-0.350649
roi_width:-0.0672073roi_height-0.747484
roi_width:-0.111804roi_height-0.269286
roi_width:-0.442428roi_height-0.355693
roi_width:0.506891roi_height0.14143
roi_width:-0.459199roi_height-0.234167
roi_width:0.410901roi_height0.24882
roi_width:0.433393roi_height0.320328
roi_width:0.329987roi_height0.242103
roi_width:0.201549roi_height0.00923049
roi_width:0.948357roi_height0.109985
roi_width:0.0768444roi_height0.168038
roi_width:-0.309393roi_height-0.196267
roi_width:-0.0371945roi_height-0.259548
roi_width:-0.094451roi_height-0.73494
roi_width:-0.0894525roi_height-0.69873
roi_width:-0.376843roi_height-0.00932956
roi_width:0.644216roi_height0.115681
roi_width:-0.269621roi_height-0.630286
roi_width:0.16531roi_height0.102166
roi_width:-0.329456roi_height-0.108267
roi_width:-0.0953994roi_height-0.00249183
roi_width:-0.0480711roi_height-0.200859
roi_width:-0.13146roi_height-0.424804
roi_width:-0.30222roi_height-0.719044
roi_width:0.190426roi_height0.785158
roi_width:0.20046roi_height0.669879
roi_width:0.415505roi_height0.249576
roi_width:-0.264349roi_height-0.044736
roi_width:-0.0583247roi_height-0.810436
roi_width:-0.458227roi_height-0.0800729
roi_width:-0.0282455roi_height-0.326877
roi_width:0.0833581roi_height0.0532144
roi_width:-0.264921roi_height-0.257292
roi_width:0.699973roi_height0.376222
roi_width:0.0225473roi_height0.34483
roi_width:0.477241roi_height0.140751
roi_width:0.210336roi_height0.415446
roi_width:-0.380884roi_height-0.888402
roi_width:-0.0789869roi_height-0.225681
roi_width:0.183234roi_height0.291045
roi_width:0.0908625roi_height0.00443485
roi_width:-0.138306roi_height-0.271731
roi_width:-0.549122roi_height-0.692605
roi_width:0.0959398roi_height0.244837
roi_width:-0.231769roi_height-0.0216734
roi_width:0.472386roi_height0.615484
roi_width:-0.029714roi_height-0.587065
roi_width:-0.413675roi_height-0.412921
roi_width:0.718187roi_height0.578364
roi_width:0.0778946roi_height0.158984
roi_width:0.483394roi_height0.14261
roi_width:-0.371653roi_height-0.245599
roi_width:0.181089roi_height0.085952
roi_width:0.254833roi_height0.488259
roi_width:-0.269268roi_height-0.189133
roi_width:0.224004roi_height0.280104
roi_width:-0.445332roi_height-0.251725
roi_width:-0.126072roi_height-0.0380536
roi_width:0.218965roi_height0.201344
roi_width:0.396684roi_height0.0166794
roi_width:-0.0334617roi_height-0.455073
roi_width:0.0883037roi_height0.222768
roi_width:0.0336958roi_height0.324449
roi_width:0.0614052roi_height0.12843
roi_width:0.539465roi_height0.479029
roi_width:-0.551632roi_height-0.0647587
roi_width:-0.145131roi_height-0.150232
roi_width:0.454778roi_height0.128781
roi_width:-0.168647roi_height-0.568086
roi_width:-0.215853roi_height-0.565256
roi_width:-0.407567roi_height-0.0281215
roi_width:0.558373roi_height0.0197553
roi_width:0.514097roi_height0.261556
roi_width:0.5651roi_height0.118261
roi_width:0.433958roi_height0.112225
roi_width:0.350871roi_height0.278138
roi_width:-0.639906roi_height-0.254659
roi_width:0.0366606roi_height0.206257
roi_width:-0.294775roi_height-0.133894
roi_width:-0.284693roi_height-0.167814
roi_width:0.050656roi_height0.108042
roi_width:-0.412291roi_height-0.151782
roi_width:-0.218743roi_height-0.208645
roi_width:0.00576445roi_height0.390635
roi_width:-0.227266roi_height-0.535031
roi_width:-0.321861roi_height-0.219096
roi_width:0.205841roi_height0.00618172
roi_width:0.220839roi_height0.382155
roi_width:-0.432637roi_height-0.148695
roi_width:-0.0219857roi_height-0.566556
roi_width:0.0469445roi_height0.116581
roi_width:-0.120276roi_height-0.417968
roi_width:0.0635084roi_height0.124334
roi_width:-0.0917391roi_height-0.364341
roi_width:0.0517729roi_height0.125272
roi_width:-0.435942roi_height-0.176153
roi_width:-0.816954roi_height-0.00292772
roi_width:-0.246862roi_height-0.222775
roi_width:0.0686848roi_height0.0613126
roi_width:-0.624851roi_height-0.277394
roi_width:0.312243roi_height0.167872
roi_width:0.158436roi_height0.149098
roi_width:-0.250017roi_height-0.252306
roi_width:-0.201831roi_height-0.700379
roi_width:0.030578roi_height0.0345622
roi_width:-0.471661roi_height-0.0692702
roi_width:0.521106roi_height0.208004
roi_width:-0.138911roi_height-0.383106
roi_width:0.182286roi_height0.52546
roi_width:-0.665003roi_height-0.338096
roi_width:0.731083roi_height0.111034
roi_width:-0.199445roi_height-0.682036
roi_width:0.152037roi_height0.0491148
roi_width:-0.251785roi_height-0.143461
roi_width:-0.352956roi_height-0.538802
roi_width:0.22303roi_height0.226149
roi_width:0.0193253roi_height0.555023
roi_width:-0.422025roi_height-0.0254385
roi_width:-0.0132661roi_height-0.0182724
roi_width:-0.0630952roi_height-0.287891
roi_width:-0.825264roi_height-0.658543
roi_width:0.486077roi_height0.289732
roi_width:0.308134roi_height0.337001
roi_width:-0.0681451roi_height-0.511883
roi_width:-0.466992roi_height-0.0642099
roi_width:0.327235roi_height0.141938
roi_width:0.0336982roi_height0.284548
roi_width:-0.257547roi_height-0.0849438
roi_width:0.477482roi_height0.242652
roi_width:-0.0611251roi_height-0.508795
roi_width:-0.113674roi_height-0.293423
roi_width:-0.870146roi_height-0.00136051
roi_width:0.00615317roi_height0.148771
roi_width:-0.871646roi_height-0.314765
roi_width:0.322985roi_height0.692376
roi_width:0.329883roi_height0.0859435
roi_width:-0.700632roi_height-1.22238
roi_width:-0.0916473roi_height-0.0930293
roi_width:0.0223892roi_height0.301234
roi_width:0.15161roi_height0.47101
roi_width:-0.0786183roi_height-0.0396814
roi_width:-0.109976roi_height-0.162868
roi_width:-0.418545roi_height-0.114149
roi_width:-0.338927roi_height-0.132396
roi_width:-0.309993roi_height-0.368196
roi_width:0.107434roi_height0.166872
roi_width:0.0210378roi_height0.569894
roi_width:-0.225805roi_height-0.355611
roi_width:0.0788767roi_height0.322876
roi_width:-0.514496roi_height-1.11633
roi_width:-0.209151roi_height-0.431416
roi_width:-0.036281roi_height-0.0204654
roi_width:-0.653639roi_height-0.135565
roi_width:-0.396582roi_height-0.265783
roi_width:0.100648roi_height0.408844
roi_width:-0.302572roi_height-0.330477
roi_width:-0.766102roi_height-0.827913
roi_width:-0.191871roi_height-0.328549
roi_width:0.477347roi_height0.0711996
roi_width:-0.132894roi_height-0.715929
roi_width:0.227365roi_height0.429298
roi_width:-0.260459roi_height-0.00393927
roi_width:0.414405roi_height0.252522
roi_width:0.061524roi_height0.157795
roi_width:-0.389185roi_height-0.172509
roi_width:-0.0582937roi_height-0.256279
roi_width:-0.00815743roi_height-0.0907529
roi_width:0.0822092roi_height0.0895079
roi_width:-0.29592roi_height-0.292278
roi_width:0.0131641roi_height0.0021829
roi_width:0.568385roi_height0.130858
roi_width:0.140375roi_height0.510718
roi_width:-0.461747roi_height-0.604973
roi_width:0.362057roi_height0.379026
roi_width:-0.0816275roi_height-0.528452
roi_width:0.0683077roi_height0.353605
roi_width:-0.170766roi_height-0.433729
roi_width:0.170215roi_height0.245383
roi_width:-0.175042roi_height-0.552494
roi_width:-0.130762roi_height-0.0198832
roi_width:0.0607578roi_height0.549242
roi_width:0.151816roi_height0.340469
roi_width:0.982002roi_height0.0127121
roi_width:-0.29451roi_height-0.132091
roi_width:-0.302056roi_height-0.466479
roi_width:0.0473412roi_height0.0190729
roi_width:-0.296133roi_height-0.0990134
roi_width:-0.142113roi_height-0.119929
roi_width:0.192668roi_height0.215507
roi_width:0.228555roi_height0.290218
roi_width:-0.514344roi_height-0.163148
roi_width:0.442411roi_height0.500616
roi_width:-0.0378258roi_height-0.492261
roi_width:-0.256681roi_height-0.551408
roi_width:-0.5891roi_height-0.344724
roi_width:0.276002roi_height0.516759
roi_width:0.210197roi_height0.309538
roi_width:0.364461roi_height0.714769
roi_width:0.0723416roi_height0.0895469
roi_width:-0.210223roi_height-0.392849
roi_width:0.116461roi_height0.0270245
roi_width:-0.250882roi_height-0.0734549
roi_width:0.227417roi_height0.239507
roi_width:-0.27298roi_height-0.453148
roi_width:0.16362roi_height0.196247
roi_width:0.170834roi_height0.180625
roi_width:-0.528838roi_height-0.488883
roi_width:0.270549roi_height0.208975
roi_width:-0.120859roi_height-0.114182
roi_width:0.428589roi_height0.118474
roi_width:-0.0099065roi_height-0.00649178
roi_width:0.289615roi_height0.0923343
roi_width:0.00221276roi_height0.732925
roi_width:-0.00604725roi_height-0.219537
roi_width:-0.155342roi_height-0.851727
roi_width:-0.221108roi_height-0.240716
roi_width:0.738731roi_height0.314901
roi_width:0.0858836roi_height0.336682
roi_width:0.0668793roi_height0.28653
roi_width:-0.39477roi_height-0.271924
roi_width:-0.178733roi_height-0.0316761
roi_width:0.0992962roi_height0.0686993
roi_width:-0.274833roi_height-0.387933
roi_width:0.244403roi_height0.740124
roi_width:0.234249roi_height0.157167
roi_width:-0.190558roi_height-0.0202344
roi_width:0.13911roi_height0.0428828
roi_width:0.348202roi_height0.27203
roi_width:0.452376roi_height0.296869
roi_width:0.0822365roi_height0.36672
roi_width:0.383812roi_height0.394869
roi_width:0.051141roi_height0.183321
roi_width:-0.492599roi_height-0.319773
roi_width:0.597449roi_height0.445239
roi_width:-0.0836796roi_height-0.172658
roi_width:-0.340797roi_height-0.944789
roi_width:0.835031roi_height0.0216666
roi_width:-0.0869778roi_height-0.200394
roi_width:-0.36364roi_height-0.771018
roi_width:-0.299001roi_height-0.469808
roi_width:0.474558roi_height0.237961
roi_width:-0.0212858roi_height-0.586542
roi_width:0.390397roi_height0.455912
roi_width:-0.200753roi_height-0.133798
roi_width:1.0272roi_height0.220389
roi_width:-0.0889105roi_height-0.16479
roi_width:0.251859roi_height0.0261976
roi_width:-0.2842roi_height-0.215095
roi_width:0.113714roi_height0.373214
roi_width:0.748761roi_height0.197159
roi_width:-0.0225699roi_height-0.445676
roi_width:0.320266roi_height0.255172
roi_width:0.0238375roi_height0.093583
roi_width:-0.401089roi_height-0.52769
roi_width:0.403501roi_height0.0520224
roi_width:-0.394697roi_height-0.0375448
roi_width:-0.52134roi_height-0.00848264
roi_width:-0.0744161roi_height-0.446808
roi_width:-0.156459roi_height-0.26531
roi_width:-0.61499roi_height-0.151204
roi_width:-0.283122roi_height-0.284646
roi_width:-0.447798roi_height-0.206175
roi_width:-0.30703roi_height-0.550644
roi_width:-0.143771roi_height-0.086143
roi_width:-0.106065roi_height-0.186868
roi_width:-0.0153385roi_height-0.0917823
roi_width:-0.225852roi_height-0.155879
roi_width:0.051869roi_height0.344671
roi_width:-0.262989roi_height-0.295341
roi_width:0.0743169roi_height0.317107
roi_width:-0.458296roi_height-0.241916
roi_width:0.431908roi_height0.312128
roi_width:0.898389roi_height0.00689557
roi_width:0.0125271roi_height0.194683
roi_width:-0.116227roi_height-0.0450494
roi_width:-0.464348roi_height-0.337982
roi_width:-0.0261931roi_height-0.068504
roi_width:0.0286481roi_height0.0132331
roi_width:0.01261roi_height0.371588
roi_width:-0.260222roi_height-0.259022
roi_width:0.16761roi_height0.241142
roi_width:-0.0728443roi_height-0.12756
roi_width:-0.707882roi_height-0.173889
roi_width:-0.104001roi_height-0.16465
roi_width:0.216786roi_height0.345411
roi_width:-1.135roi_height-0.242295
roi_width:-0.428104roi_height-0.354087
roi_width:0.792521roi_height0.337602
roi_width:-0.445023roi_height-0.137621
roi_width:0.0918899roi_height0.117258
roi_width:0.120033roi_height0.171224
roi_width:0.30861roi_height0.071969
roi_width:-0.0148083roi_height-0.347946
roi_width:-0.245618roi_height-0.0540899
roi_width:0.0720531roi_height0.0397399
roi_width:-0.421661roi_height-0.231051
roi_width:0.0395415roi_height0.176139
roi_width:-0.267395roi_height-0.577485
roi_width:0.26518roi_height0.156114
roi_width:-0.561466roi_height-0.477042
roi_width:0.347202roi_height0.205426
roi_width:-0.172857roi_height-0.289412
roi_width:0.698551roi_height0.0921447
roi_width:0.102256roi_height0.281261
roi_width:0.0766093roi_height0.376005
roi_width:-0.209463roi_height-0.494328
roi_width:-0.221333roi_height-0.257274
roi_width:0.112744roi_height0.0331973
roi_width:-0.0645053roi_height-0.0688829
roi_width:-0.317488roi_height-0.392802
roi_width:-0.0175961roi_height-0.493412
roi_width:-0.715035roi_height-0.0660552
roi_width:-0.388039roi_height-0.0588973
roi_width:0.399482roi_height0.168232
roi_width:0.222885roi_height0.100168
roi_width:0.156464roi_height0.00690687
roi_width:-0.267699roi_height-0.156143
roi_width:0.147979roi_height0.248031
roi_width:0.233456roi_height0.408586
roi_width:-0.00720209roi_height-0.447496
roi_width:-0.467951roi_height-0.777371
roi_width:-0.311464roi_height-0.0994589
roi_width:0.214951roi_height0.8584
roi_width:0.653153roi_height0.222348
roi_width:-0.0973344roi_height-0.435282
roi_width:-0.388845roi_height-0.0570643
roi_width:0.0940124roi_height0.436449
roi_width:0.524018roi_height0.32139
roi_width:0.25007roi_height0.0314492
roi_width:-0.391419roi_height-0.173375
roi_width:0.197935roi_height0.225283
roi_width:-0.53991roi_height-0.0569916
roi_width:0.434492roi_height0.123476
roi_width:0.0351439roi_height0.44046
roi_width:-0.347214roi_height-0.00239208
roi_width:0.224764roi_height0.732544
roi_width:-0.105223roi_height-0.0838021
roi_width:0.0996937roi_height0.156132
roi_width:-0.498101roi_height-0.11056
roi_width:-0.23077roi_height-0.180307
roi_width:0.117199roi_height0.194437
roi_width:-0.0796048roi_height-0.68464
roi_width:-0.307048roi_height-0.569808
roi_width:-0.114038roi_height-0.381857
roi_width:-0.824798roi_height-0.0431581
roi_width:-0.318624roi_height-0.329799
roi_width:-0.112112roi_height-0.593551
roi_width:-0.792285roi_height-0.378192
roi_width:-0.694669roi_height-0.00613183
roi_width:-0.105673roi_height-1.01601
roi_width:-0.608046roi_height-0.022254
roi_width:0.179664roi_height0.38544
roi_width:-0.250911roi_height-0.547025
roi_width:-0.469374roi_height-0.297748
roi_width:-0.435052roi_height-0.0093165
roi_width:0.00687718roi_height0.316287
roi_width:-0.142045roi_height-0.0828197
roi_width:-0.172616roi_height-0.16279

Computational complexity:       209.87 GMac
Number of parameters:           69.11 M 
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
Warning: variables __flops__ or __params__ are already defined for the moduleGELU ptflops can affect your code!
MaskRCNN(
  37.28 M, 53.947% Params, 209.87 GMac, 100.000% MACs, 
  (backbone): SwinTransformer(
    17.29 M, 25.022% Params, 58.46 GMac, 27.856% MACs, 
    (patch_embed): PatchEmbed(
      4.7 k, 0.007% Params, 263.42 MMac, 0.126% MACs, 
      (adap_padding): AdaptivePadding(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
      (projection): Conv2d(4.7 k, 0.007% Params, 263.42 MMac, 0.126% MACs, 3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (96,), eps=1e-05, elementwise_affine=True)
    )
    (drop_after_pos): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
    (stages): ModuleList(
      17.29 M, 25.015% Params, 58.2 GMac, 27.731% MACs, 
      (0): SwinBlockSequence(
        148.22 k, 0.214% Params, 5.31 GMac, 2.530% MACs, 
        (blocks): ModuleList(
          74.5 k, 0.108% Params, 4.28 GMac, 2.038% MACs, 
          (0): SwinBlock(
            37.25 k, 0.054% Params, 2.14 GMac, 1.019% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              37.25 k, 0.054% Params, 2.1 GMac, 0.998% MACs, 
              (w_msa): WindowMSA(
                37.25 k, 0.054% Params, 2.1 GMac, 0.998% MACs, 
                (qkv): Linear(27.94 k, 0.040% Params, 1.57 GMac, 0.749% MACs, in_features=96, out_features=288, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(9.31 k, 0.013% Params, 523.84 MMac, 0.250% MACs, in_features=96, out_features=96, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 43.01 MMac, 0.020% MACs, 
              (activate): GELU(0, 0.000% Params, 21.5 MMac, 0.010% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 21.5 MMac, 0.010% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 21.5 MMac, 0.010% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=96, out_features=384, bias=True)
                  (1): GELU(0, 0.000% Params, 21.5 MMac, 0.010% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=96, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (1): SwinBlock(
            37.25 k, 0.054% Params, 2.14 GMac, 1.019% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              37.25 k, 0.054% Params, 2.1 GMac, 0.998% MACs, 
              (w_msa): WindowMSA(
                37.25 k, 0.054% Params, 2.1 GMac, 0.998% MACs, 
                (qkv): Linear(27.94 k, 0.040% Params, 1.57 GMac, 0.749% MACs, in_features=96, out_features=288, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(9.31 k, 0.013% Params, 523.84 MMac, 0.250% MACs, in_features=96, out_features=96, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 43.01 MMac, 0.020% MACs, 
              (activate): GELU(0, 0.000% Params, 21.5 MMac, 0.010% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 21.5 MMac, 0.010% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 21.5 MMac, 0.010% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=96, out_features=384, bias=True)
                  (1): GELU(0, 0.000% Params, 21.5 MMac, 0.010% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=96, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
        )
        (downsample): PatchMerging(
          73.73 k, 0.107% Params, 1.03 GMac, 0.492% MACs, 
          (adap_padding): AdaptivePadding(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          (sampler): Unfold(0, 0.000% Params, 0.0 Mac, 0.000% MACs, kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
          (norm): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(73.73 k, 0.107% Params, 1.03 GMac, 0.492% MACs, in_features=384, out_features=192, bias=False)
        )
      )
      (1): SwinBlockSequence(
        591.36 k, 0.856% Params, 5.41 GMac, 2.578% MACs, 
        (blocks): ModuleList(
          296.45 k, 0.429% Params, 4.38 GMac, 2.086% MACs, 
          (0): SwinBlock(
            148.22 k, 0.214% Params, 2.19 GMac, 1.043% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (192,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              148.22 k, 0.214% Params, 2.17 GMac, 1.033% MACs, 
              (w_msa): WindowMSA(
                148.22 k, 0.214% Params, 2.17 GMac, 1.033% MACs, 
                (qkv): Linear(111.17 k, 0.161% Params, 1.63 GMac, 0.775% MACs, in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(37.06 k, 0.054% Params, 541.9 MMac, 0.258% MACs, in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (192,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 21.5 MMac, 0.010% MACs, 
              (activate): GELU(0, 0.000% Params, 10.75 MMac, 0.005% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=192, out_features=768, bias=True)
                  (1): GELU(0, 0.000% Params, 10.75 MMac, 0.005% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=768, out_features=192, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (1): SwinBlock(
            148.22 k, 0.214% Params, 2.19 GMac, 1.043% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (192,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              148.22 k, 0.214% Params, 2.17 GMac, 1.033% MACs, 
              (w_msa): WindowMSA(
                148.22 k, 0.214% Params, 2.17 GMac, 1.033% MACs, 
                (qkv): Linear(111.17 k, 0.161% Params, 1.63 GMac, 0.775% MACs, in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(37.06 k, 0.054% Params, 541.9 MMac, 0.258% MACs, in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (192,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 21.5 MMac, 0.010% MACs, 
              (activate): GELU(0, 0.000% Params, 10.75 MMac, 0.005% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=192, out_features=768, bias=True)
                  (1): GELU(0, 0.000% Params, 10.75 MMac, 0.005% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=768, out_features=192, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
        )
        (downsample): PatchMerging(
          294.91 k, 0.427% Params, 1.03 GMac, 0.492% MACs, 
          (adap_padding): AdaptivePadding(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          (sampler): Unfold(0, 0.000% Params, 0.0 Mac, 0.000% MACs, kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
          (norm): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (768,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(294.91 k, 0.427% Params, 1.03 GMac, 0.492% MACs, in_features=768, out_features=384, bias=False)
        )
      )
      (2): SwinBlockSequence(
        11.82 M, 17.109% Params, 42.84 GMac, 20.414% MACs, 
        (blocks): ModuleList(
          10.64 M, 15.402% Params, 41.81 GMac, 19.923% MACs, 
          (0): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (1): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (2): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (3): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (4): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (5): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (6): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (7): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (8): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (9): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (10): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (11): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (12): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (13): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (14): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (15): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (16): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (17): SwinBlock(
            591.36 k, 0.856% Params, 2.32 GMac, 1.107% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                591.36 k, 0.856% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(443.52 k, 0.642% Params, 1.73 GMac, 0.826% MACs, in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(147.84 k, 0.214% Params, 578.03 MMac, 0.275% MACs, in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 10.75 MMac, 0.005% MACs, 
              (activate): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=384, out_features=1536, bias=True)
                  (1): GELU(0, 0.000% Params, 5.38 MMac, 0.003% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=1536, out_features=384, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
        )
        (downsample): PatchMerging(
          1.18 M, 1.707% Params, 1.03 GMac, 0.492% MACs, 
          (adap_padding): AdaptivePadding(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
          (sampler): Unfold(0, 0.000% Params, 0.0 Mac, 0.000% MACs, kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
          (norm): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (1536,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(1.18 M, 1.707% Params, 1.03 GMac, 0.492% MACs, in_features=1536, out_features=768, bias=False)
        )
      )
      (3): SwinBlockSequence(
        4.72 M, 6.836% Params, 4.63 GMac, 2.209% MACs, 
        (blocks): ModuleList(
          4.72 M, 6.836% Params, 4.63 GMac, 2.209% MACs, 
          (0): SwinBlock(
            2.36 M, 3.418% Params, 2.32 GMac, 1.104% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              2.36 M, 3.418% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                2.36 M, 3.418% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(1.77 M, 2.564% Params, 1.73 GMac, 0.826% MACs, in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(590.59 k, 0.855% Params, 578.03 MMac, 0.275% MACs, in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
              (activate): GELU(0, 0.000% Params, 2.69 MMac, 0.001% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 2.69 MMac, 0.001% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 2.69 MMac, 0.001% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=768, out_features=3072, bias=True)
                  (1): GELU(0, 0.000% Params, 2.69 MMac, 0.001% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=3072, out_features=768, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
          (1): SwinBlock(
            2.36 M, 3.418% Params, 2.32 GMac, 1.104% MACs, 
            (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              2.36 M, 3.418% Params, 2.31 GMac, 1.102% MACs, 
              (w_msa): WindowMSA(
                2.36 M, 3.418% Params, 2.31 GMac, 1.102% MACs, 
                (qkv): Linear(1.77 M, 2.564% Params, 1.73 GMac, 0.826% MACs, in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (proj): Linear(590.59 k, 0.855% Params, 578.03 MMac, 0.275% MACs, in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                (softmax): Softmax(0, 0.000% Params, 0.0 Mac, 0.000% MACs, dim=-1)
              )
              (drop): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
            (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              0, 0.000% Params, 5.38 MMac, 0.003% MACs, 
              (activate): GELU(0, 0.000% Params, 2.69 MMac, 0.001% MACs, )
              (layers): Sequential(
                0, 0.000% Params, 2.69 MMac, 0.001% MACs, 
                (0): Sequential(
                  0, 0.000% Params, 2.69 MMac, 0.001% MACs, 
                  (0): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=768, out_features=3072, bias=True)
                  (1): GELU(0, 0.000% Params, 2.69 MMac, 0.001% MACs, )
                  (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
                )
                (1): Linear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=3072, out_features=768, bias=True)
                (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
              )
              (dropout_layer): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
            )
          )
        )
      )
    )
    (norm0): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (96,), eps=1e-05, elementwise_affine=True)
    (norm1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (192,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (384,), eps=1e-05, elementwise_affine=True)
    (norm3): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (768,), eps=1e-05, elementwise_affine=True)
  )
  (neck): FPN(
    2.73 M, 3.950% Params, 46.49 GMac, 22.150% MACs, 
    (lateral_convs): ModuleList(
      369.66 k, 0.535% Params, 2.6 GMac, 1.239% MACs, 
      (0): ConvModule(
        24.83 k, 0.036% Params, 1.39 GMac, 0.663% MACs, 
        (conv): Conv2d(24.83 k, 0.036% Params, 1.39 GMac, 0.663% MACs, 96, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        49.41 k, 0.071% Params, 691.71 MMac, 0.330% MACs, 
        (conv): Conv2d(49.41 k, 0.071% Params, 691.71 MMac, 0.330% MACs, 192, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        98.56 k, 0.143% Params, 344.96 MMac, 0.164% MACs, 
        (conv): Conv2d(98.56 k, 0.143% Params, 344.96 MMac, 0.164% MACs, 384, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): ConvModule(
        196.86 k, 0.285% Params, 172.26 MMac, 0.082% MACs, 
        (conv): Conv2d(196.86 k, 0.285% Params, 172.26 MMac, 0.082% MACs, 768, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      2.36 M, 3.415% Params, 43.89 GMac, 20.912% MACs, 
      (0): ConvModule(
        590.08 k, 0.854% Params, 33.04 GMac, 15.745% MACs, 
        (conv): Conv2d(590.08 k, 0.854% Params, 33.04 GMac, 15.745% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        590.08 k, 0.854% Params, 8.26 GMac, 3.936% MACs, 
        (conv): Conv2d(590.08 k, 0.854% Params, 8.26 GMac, 3.936% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        590.08 k, 0.854% Params, 2.07 GMac, 0.984% MACs, 
        (conv): Conv2d(590.08 k, 0.854% Params, 2.07 GMac, 0.984% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        590.08 k, 0.854% Params, 516.32 MMac, 0.246% MACs, 
        (conv): Conv2d(590.08 k, 0.854% Params, 516.32 MMac, 0.246% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (rpn_head): RPNHead(
    593.93 k, 0.859% Params, 44.31 GMac, 21.114% MACs, 
    (loss_cls): CrossEntropyLoss(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
    (loss_bbox): L1Loss(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
    (rpn_conv): Conv2d(590.08 k, 0.854% Params, 44.03 GMac, 20.977% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (rpn_cls): Conv2d(771, 0.001% Params, 57.52 MMac, 0.027% MACs, 256, 3, kernel_size=(1, 1), stride=(1, 1))
    (rpn_reg): Conv2d(3.08 k, 0.004% Params, 230.09 MMac, 0.110% MACs, 256, 12, kernel_size=(1, 1), stride=(1, 1))
  )
  init_cfg={'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}
  (roi_head): StandardRoIHead(
    16.67 M, 24.116% Params, 60.61 GMac, 28.879% MACs, 
    (bbox_roi_extractor): SingleRoIExtractor(
      0, 0.000% Params, 0.0 Mac, 0.000% MACs, 
      (roi_layers): ModuleList(
        0, 0.000% Params, 0.0 Mac, 0.000% MACs, 
        (0): RoIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
        (1): RoIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
        (2): RoIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
        (3): RoIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
      )
    )
    (bbox_head): Shared2FCBBoxHead(
      14.31 M, 20.701% Params, 14.31 GMac, 6.817% MACs, 
      (loss_cls): CrossEntropyLoss(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
      (loss_bbox): L1Loss(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
      (fc_cls): Linear(83.03 k, 0.120% Params, 82.94 MMac, 0.040% MACs, in_features=1024, out_features=81, bias=True)
      (fc_reg): Linear(328.0 k, 0.475% Params, 327.68 MMac, 0.156% MACs, in_features=1024, out_features=320, bias=True)
      (shared_convs): ModuleList(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
      (shared_fcs): ModuleList(
        13.9 M, 20.106% Params, 13.89 GMac, 6.620% MACs, 
        (0): Linear(12.85 M, 18.587% Params, 12.85 GMac, 6.120% MACs, in_features=12544, out_features=1024, bias=True)
        (1): Linear(1.05 M, 1.519% Params, 1.05 GMac, 0.500% MACs, in_features=1024, out_features=1024, bias=True)
      )
      (cls_convs): ModuleList(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
      (cls_fcs): ModuleList(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
      (reg_convs): ModuleList(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
      (reg_fcs): ModuleList(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
      (relu): ReLU(0, 0.000% Params, 2.05 MMac, 0.001% MACs, inplace=True)
    )
    init_cfg=[{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}, {'type': 'Xavier', 'distribution': 'uniform', 'override': [{'name': 'shared_fcs'}, {'name': 'cls_fcs'}, {'name': 'reg_fcs'}]}]
    (mask_roi_extractor): SingleRoIExtractor(
      0, 0.000% Params, 0.0 Mac, 0.000% MACs, 
      (roi_layers): ModuleList(
        0, 0.000% Params, 0.0 Mac, 0.000% MACs, 
        (0): RoIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
        (1): RoIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
        (2): RoIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
        (3): RoIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
      )
    )
    (mask_head): FCNMaskHead(
      2.36 M, 3.415% Params, 46.3 GMac, 22.062% MACs, 
      (loss_mask): CrossEntropyLoss(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
      (convs): ModuleList(
        2.36 M, 3.415% Params, 46.28 GMac, 22.053% MACs, 
        (0): ConvModule(
          590.08 k, 0.854% Params, 11.57 GMac, 5.513% MACs, 
          (conv): Conv2d(590.08 k, 0.854% Params, 11.57 GMac, 5.511% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): ReLU(0, 0.000% Params, 5.02 MMac, 0.002% MACs, inplace=True)
        )
        (1): ConvModule(
          590.08 k, 0.854% Params, 11.57 GMac, 5.513% MACs, 
          (conv): Conv2d(590.08 k, 0.854% Params, 11.57 GMac, 5.511% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): ReLU(0, 0.000% Params, 5.02 MMac, 0.002% MACs, inplace=True)
        )
        (2): ConvModule(
          590.08 k, 0.854% Params, 11.57 GMac, 5.513% MACs, 
          (conv): Conv2d(590.08 k, 0.854% Params, 11.57 GMac, 5.511% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): ReLU(0, 0.000% Params, 5.02 MMac, 0.002% MACs, inplace=True)
        )
        (3): ConvModule(
          590.08 k, 0.854% Params, 11.57 GMac, 5.513% MACs, 
          (conv): Conv2d(590.08 k, 0.854% Params, 11.57 GMac, 5.511% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (activate): ReLU(0, 0.000% Params, 5.02 MMac, 0.002% MACs, inplace=True)
        )
      )
      (upsample): ConvTranspose2d(0, 0.000% Params, 0.0 Mac, 0.000% MACs, 256, 256, kernel_size=(2, 2), stride=(2, 2))
      (conv_logits): Conv2d(0, 0.000% Params, 0.0 Mac, 0.000% MACs, 256, 80, kernel_size=(1, 1), stride=(1, 1))
      (relu): ReLU(0, 0.000% Params, 20.07 MMac, 0.010% MACs, inplace=True)
    )
  )
)
==============================
Input shape: (3, 800, 1120)
Flops: 209.87 GMac
Params: 69.11 M
==============================
!!!Please be cautious if you use the results in papers. You may need to check if all ops are supported and verify that the flops computation is correct.
